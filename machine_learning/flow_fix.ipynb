{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56600520",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807c43fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbook/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\"csebuetnlp/xlsum\",'indonesian')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks, models, layers, preprocessing as kprocessing #(2.6.0)\n",
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91de7722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4509</th>\n",
       "      <td>Mumbai, Feb 23 (PTI) Fruit juice concentrate m...</td>\n",
       "      <td>Fruit juice concentrate maker Rasna is eyeing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4510</th>\n",
       "      <td>Former cricketer Sachin Tendulkar was spotted ...</td>\n",
       "      <td>Former Indian cricketer Sachin Tendulkar atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4511</th>\n",
       "      <td>Aamir Khan, whose last film Dangal told the st...</td>\n",
       "      <td>Aamir Khan, while talking about reality shows ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4512</th>\n",
       "      <td>Maharahstra Power Minister Chandrashekhar Bawa...</td>\n",
       "      <td>The Maharashtra government has initiated an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>More than half of the languages spoken by Indi...</td>\n",
       "      <td>At least 400 languages or more than half langu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4514 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     The Daman and Diu administration on Wednesday ...   \n",
       "1     From her special numbers to TV?appearances, Bo...   \n",
       "2     The Indira Gandhi Institute of Medical Science...   \n",
       "3     Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4     Hotels in Mumbai and other Indian cities are t...   \n",
       "...                                                 ...   \n",
       "4509  Mumbai, Feb 23 (PTI) Fruit juice concentrate m...   \n",
       "4510  Former cricketer Sachin Tendulkar was spotted ...   \n",
       "4511  Aamir Khan, whose last film Dangal told the st...   \n",
       "4512  Maharahstra Power Minister Chandrashekhar Bawa...   \n",
       "4513  More than half of the languages spoken by Indi...   \n",
       "\n",
       "                                                summary  \n",
       "0     The Administration of Union Territory Daman an...  \n",
       "1     Malaika Arora slammed an Instagram user who tr...  \n",
       "2     The Indira Gandhi Institute of Medical Science...  \n",
       "3     Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4     Hotels in Maharashtra will train their staff t...  \n",
       "...                                                 ...  \n",
       "4509  Fruit juice concentrate maker Rasna is eyeing ...  \n",
       "4510  Former Indian cricketer Sachin Tendulkar atten...  \n",
       "4511  Aamir Khan, while talking about reality shows ...  \n",
       "4512  The Maharashtra government has initiated an in...  \n",
       "4513  At least 400 languages or more than half langu...  \n",
       "\n",
       "[4514 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame with explicit encoding\n",
    "dataset = pd.read_csv(\"news_summary.csv\", encoding='latin1')\n",
    "\n",
    "# Access the 'text' column of the first row in the 'ctext' column\n",
    "df = pd.DataFrame()\n",
    "df['text'] = dataset['ctext']\n",
    "df['summary'] = dataset['text']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931370dc",
   "metadata": {},
   "source": [
    "# Data Cleansing using RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6e1a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text column\n",
    "i = 0\n",
    "for text in df['text']:\n",
    "    if isinstance(text, str):  # Check if the element is a string\n",
    "        df['text'][i] = re.sub(r\"[!\\\"#\\$%&\\(\\)\\*\\+,-\\./:;<=>\\?@\\[\\]\\^_`{\\|}~]\", \"\", text.lower())\n",
    "    i += 1\n",
    "\n",
    "# Clean summary column\n",
    "i = 0\n",
    "for text in df['summary']:\n",
    "    if isinstance(text, str):  # Check if the element is a string\n",
    "        df['summary'][i] = re.sub(r\"[!\\\"#\\$%&\\(\\)\\*\\+,-\\./:;<=>\\?@\\[\\]\\^_`{\\|}~]\", \"\", text.lower())\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f7f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words(\"english\")\n",
    "\n",
    "# Clean stopwords in the text column\n",
    "i = 0\n",
    "for text in df['text']:\n",
    "    if isinstance(text, str):  # Check if the element is a string\n",
    "        temp = \"\"\n",
    "        text = text.split(\" \")\n",
    "        for word in text:\n",
    "            if word not in stopword:\n",
    "                temp = temp + \" \" + word\n",
    "        df['text'][i] = temp.strip()  # strip to remove leading/trailing spaces\n",
    "    i += 1\n",
    "\n",
    "# Clean stopwords in the summary column\n",
    "i = 0\n",
    "for text in df['summary']:\n",
    "    if isinstance(text, str):  # Check if the element is a string\n",
    "        temp = \"\"\n",
    "        text = text.split(\" \")\n",
    "        for word in text:\n",
    "            if word not in stopword:\n",
    "                temp = temp + \" \" + word\n",
    "        df['summary'][i] = temp.strip()  # strip to remove leading/trailing spaces\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d5adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.dropna(axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d13c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_text = []\n",
    "selected_sum = []\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    if 150 < len(df1['text'][i].split(' ')) < 800:\n",
    "        if 10 < len(df1['summary'][i].split(' ')) < 50:\n",
    "            selected_text.append(df1['text'][i])\n",
    "            selected_sum.append(df1['summary'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "429c2f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daman diu administration wednesday withdrew ci...</td>\n",
       "      <td>administration union territory daman diu revok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>special numbers tvappearances bollywood actor ...</td>\n",
       "      <td>malaika arora slammed instagram user trolled d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indira gandhi institute medical sciences igims...</td>\n",
       "      <td>indira gandhi institute medical sciences igims...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lashkaretaiba's kashmir commander abu dujana k...</td>\n",
       "      <td>lashkaretaiba's kashmir commander abu dujana k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hotels mumbai indian cities train staff spot s...</td>\n",
       "      <td>hotels maharashtra train staff spot signs sex ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  daman diu administration wednesday withdrew ci...   \n",
       "1  special numbers tvappearances bollywood actor ...   \n",
       "2  indira gandhi institute medical sciences igims...   \n",
       "3  lashkaretaiba's kashmir commander abu dujana k...   \n",
       "4  hotels mumbai indian cities train staff spot s...   \n",
       "\n",
       "                                             summary  \n",
       "0  administration union territory daman diu revok...  \n",
       "1  malaika arora slammed instagram user trolled d...  \n",
       "2  indira gandhi institute medical sciences igims...  \n",
       "3  lashkaretaiba's kashmir commander abu dujana k...  \n",
       "4  hotels maharashtra train staff spot signs sex ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2['text'] = selected_text\n",
    "df2['summary'] = selected_sum\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c78ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1976 training samples.\n",
      "We have 495 validation samples.\n"
     ]
    }
   ],
   "source": [
    "test_index = int(0.8 * len(df2))\n",
    "\n",
    "train_data = df2[:test_index]\n",
    "test_data = df2[test_index:]\n",
    "\n",
    "print(f\"We have {len(train_data)} training samples.\")\n",
    "print(f\"We have {len(test_data)} validation samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98f3e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xstartx central bureau investigation cbi formed special investigation team sit saturday investigate rape murder case minor girl shimla also registered two firs case one girl death one accused police custody xendx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ht/89vfnm2574sb63gbkp_tj13h0000gn/T/ipykernel_10490/3064563492.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['summary'] = train_data['summary'].apply(lambda x: special_tokens[0]+' '+str(x)+' '+special_tokens[1] if isinstance(x, str) else x)\n",
      "/var/folders/ht/89vfnm2574sb63gbkp_tj13h0000gn/T/ipykernel_10490/3064563492.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['summary'] = test_data['summary'].apply(lambda x: special_tokens[0]+' '+str(x)+' '+special_tokens[1] if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# Add START and END tokens to the summaries (y)\n",
    "special_tokens = (\"xstartx\", \"xendx\")\n",
    "\n",
    "# Handle NaN or float values in \"x_sum\" column\n",
    "train_data['summary'] = train_data['summary'].apply(lambda x: special_tokens[0]+' '+str(x)+' '+special_tokens[1] if isinstance(x, str) else x)\n",
    "test_data['summary'] = test_data['summary'].apply(lambda x: special_tokens[0]+' '+str(x)+' '+special_tokens[1] if isinstance(x, str) else x)\n",
    "\n",
    "# Accessing a specific element\n",
    "i = 0\n",
    "print(train_data['summary'][500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7b49faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.array(train_data['text'])\n",
    "y_train = np.array(train_data['summary'])\n",
    "x_test = np.array(test_data['text'])\n",
    "y_test = np.array(test_data['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ccd6d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delhi chief minister arvind kejriwal sunday denied rift senior leader kumar vishwas aap mla accused latter trying break party bjp's behestaam aadmi party mla amanatullah khan alleged kumar vishwas conspiring break party asked legislators join bjp offer rs 30 crore eachthis prompted kejriwal say attempts made create rift kumar vishwas one founders aap longtime friend kejriwal deputy manish sisodiakhan circulated whatsapp message saying kumar vishwas called aap mlas home monetary offer behest bharatiya janata partykumar vishwas called mlas asked made party convenor said legislator okhla south delhi kumar vishwas gave second offer join bjp ready pay rs 30 crore one themi think done behest bjp khan said adding four aap mlas tasked arranging meetings party legislators kumar vishwasthe four mlas also held meeting unnamed minister khan said citing 10 unnamed mlas revealed kumar vishwas like brother kejriwal tweeted kumar vishwas like younger brothersome people trying show gulf us people enemies party mend ways nobody separate uskhan confirmed ians circulated message kumar vishwas wants usurp break aapin tv interview friday kumar vishwas said party hesitate taking call change leadership poor show delhi municipal pollsthe aap finished much behind bjp three wings municipal corporation congress finished bottom heap said blaming evms entirely poll defeats wrong mistrust among people visavis partywatch video\n",
      "\n",
      "xstartx delhi cm arvind kejriwal sunday denied rift kumar vishwas said aap leader like younger brother kejriwal added people trying show rift two nobody could separate comes aap mla amanatullah khan alleged kumar conspiring break party xendx\n"
     ]
    }
   ],
   "source": [
    "print(x_test[14])\n",
    "print('\\n' + y_test[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795aa0f4",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d688299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57896 words\n",
      "1976 sequences of length 500\n"
     ]
    }
   ],
   "source": [
    "## Tokenize text\n",
    "num_word = 6000\n",
    "tokenizer = kprocessing.text.Tokenizer(num_words=num_word, lower=True, split=' ', oov_token=None)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "art_vocabulary = {0:\"<PAD>\"}\n",
    "art_vocabulary.update(tokenizer.index_word)\n",
    "print(len(art_vocabulary), \"words\")\n",
    "\n",
    "## Create sequence\n",
    "art_text2seq = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "## Padding sequence\n",
    "X_train = kprocessing.sequence.pad_sequences(art_text2seq,\n",
    "                                             maxlen=500, padding='post', truncating=\"post\")\n",
    "\n",
    "print(X_train.shape[0], \"sequences of length\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faa62f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66132 words\n",
      "495 sequences of length 500\n"
     ]
    }
   ],
   "source": [
    "## tokenize text\n",
    "tokenizer.fit_on_texts(x_test)\n",
    "art_vocabulary.update(tokenizer.index_word)\n",
    "print(len(art_vocabulary), \"words\")\n",
    "## create sequence\n",
    "art_text2seq = tokenizer.texts_to_sequences(x_test)\n",
    "## padding sequence\n",
    "X_test = kprocessing.sequence.pad_sequences(art_text2seq,\n",
    "                                             maxlen=X_train.shape[1], padding='post', truncating=\"post\")\n",
    "print(X_test.shape[0], \"sequences of length\", X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8dd32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "x_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=25000) \n",
    "x_tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "x_train_seq = x_tokenizer.texts_to_sequences(x_train) \n",
    "x_test_seq = x_tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad zero upto maximum length\n",
    "X_train = tf.keras.utils.pad_sequences(x_train_seq,  maxlen=800, padding='post')\n",
    "X_test = tf.keras.utils.pad_sequences(x_test_seq, maxlen=800, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95d9cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare a tokenizer, again -- by not considering the rare words\n",
    "y_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=8000) \n",
    "y_tokenizer.fit_on_texts(y_train)\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "y_train_seq = y_tokenizer.texts_to_sequences(y_train) \n",
    "y_test_seq = y_tokenizer.texts_to_sequences(y_test) \n",
    "\n",
    "# Pad zero upto maximum length\n",
    "Y_train = tf.keras.utils.pad_sequences(y_train_seq, maxlen=50, padding='post')\n",
    "Y_test = tf.keras.utils.pad_sequences(y_test_seq, maxlen=50, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "# y_voc = y_tokenizer.num_words + 1\n",
    "\n",
    "# print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b0ec4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab = x_tokenizer.index_word\n",
    "sum_vocab = y_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89f1fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = x_tokenizer.to_json() \n",
    "\n",
    "with open('word_corpus.json', 'w', encoding='utf-8') as f:  \n",
    "      f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1619cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = y_tokenizer.to_json() \n",
    "\n",
    "with open('label_corpus.json', 'w', encoding='utf-8') as f:  \n",
    "      f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ae2dc",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47e9eee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/macbook/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/macbook/anaconda3/lib/python3.11/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/macbook/anaconda3/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/macbook/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /Users/macbook/anaconda3/lib/python3.11/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pandas in /Users/macbook/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in /Users/macbook/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/macbook/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/macbook/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: simpful in /Users/macbook/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.11.1)\n",
      "Requirement already satisfied: fst-pso in /Users/macbook/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/macbook/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in /Users/macbook/anaconda3/lib/python3.11/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "import gensim.downloader\n",
    "nlp = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3110a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "text_embeddings = np.zeros((len(text_vocab)+1, nlp.vector_size), dtype='float32')\n",
    "\n",
    "for word,idx in text_vocab.items():\n",
    "  ## update the row with vector\n",
    "  try:\n",
    "    text_embeddings[idx] = nlp[word]\n",
    "  except:\n",
    "  ## if word not in model then skip and the row stays all 0s\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29816471",
   "metadata": {},
   "outputs": [],
   "source": [
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "sum_embeddings = np.zeros((len(sum_vocab)+1, nlp.vector_size), dtype='float32')\n",
    "\n",
    "for word,idx in sum_vocab.items():\n",
    "  ## update the row with vector\n",
    "  try:\n",
    "    sum_embeddings[idx] = nlp[word]\n",
    "  except:\n",
    "  ## if word not in model then skip and the row stays all 0s\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5a2af",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5688effd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57895"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape[0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b08a7c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 800)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 800, 100)             5789600   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, None, 100)            1478600   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 800, 128),           117248    ['embedding[0][0]']           \n",
      "                              (None, 128),                                                        \n",
      "                              (None, 128)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, None, 128),          117248    ['embedding_1[0][0]',         \n",
      "                              (None, 128),                           'lstm[0][1]',                \n",
      "                              (None, 128)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 14785)          1907265   ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9409961 (35.90 MB)\n",
      "Trainable params: 9409961 (35.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Encoder\n",
    "lstm_units = 128  # Reduced the number of LSTM units\n",
    "embeddings_size = 100  # Reduced the embedding size\n",
    "\n",
    "encoder_inputs = Input(shape=(X_train.shape[1],))\n",
    "encoder_embedding = Embedding(input_dim=text_embeddings.shape[0], output_dim=embeddings_size,\n",
    "                               weights=[text_embeddings[:, :embeddings_size]], trainable=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units=lstm_units, dropout=0.2, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=sum_embeddings.shape[0], output_dim=embeddings_size,\n",
    "                                weights=[sum_embeddings[:, :embeddings_size]], trainable=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(units=lstm_units, dropout=0.2, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# Dense Layer\n",
    "dense_layer = Dense(units=len(sum_vocab), activation='softmax')\n",
    "outputs = dense_layer(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.01), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b509d847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 35s 2s/step - loss: 7.3394 - accuracy: 0.3190 - val_loss: 6.4682 - val_accuracy: 0.3285\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 35s 2s/step - loss: 6.0992 - accuracy: 0.3190 - val_loss: 5.9375 - val_accuracy: 0.3294\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 34s 2s/step - loss: 5.6086 - accuracy: 0.3315 - val_loss: 5.5596 - val_accuracy: 0.3552\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 41s 2s/step - loss: 5.3738 - accuracy: 0.3467 - val_loss: 5.5472 - val_accuracy: 0.3567\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 39s 2s/step - loss: 5.2725 - accuracy: 0.3486 - val_loss: 5.5584 - val_accuracy: 0.3566\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 33s 2s/step - loss: 5.1959 - accuracy: 0.3502 - val_loss: 5.5728 - val_accuracy: 0.3569\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 35s 2s/step - loss: 5.1100 - accuracy: 0.3519 - val_loss: 5.5832 - val_accuracy: 0.3548\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 35s 2s/step - loss: 5.0180 - accuracy: 0.3539 - val_loss: 5.5985 - val_accuracy: 0.3576\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 34s 2s/step - loss: 4.9107 - accuracy: 0.3556 - val_loss: 5.6309 - val_accuracy: 0.3587\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 33s 2s/step - loss: 4.7913 - accuracy: 0.3600 - val_loss: 5.6531 - val_accuracy: 0.3600\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 33s 2s/step - loss: 4.6609 - accuracy: 0.3648 - val_loss: 5.6839 - val_accuracy: 0.3641\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 35s 2s/step - loss: 4.5270 - accuracy: 0.3706 - val_loss: 5.7232 - val_accuracy: 0.3635\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 34s 2s/step - loss: 4.3946 - accuracy: 0.3766 - val_loss: 5.7590 - val_accuracy: 0.3681\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 37s 2s/step - loss: 4.2583 - accuracy: 0.3822 - val_loss: 5.8040 - val_accuracy: 0.3682\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 40s 2s/step - loss: 4.1241 - accuracy: 0.3890 - val_loss: 5.8528 - val_accuracy: 0.3712\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 42s 2s/step - loss: 3.9789 - accuracy: 0.3975 - val_loss: 5.8956 - val_accuracy: 0.3705\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 39s 2s/step - loss: 3.8538 - accuracy: 0.4062 - val_loss: 5.9333 - val_accuracy: 0.3727\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 41s 2s/step - loss: 3.9079 - accuracy: 0.4045 - val_loss: 5.9077 - val_accuracy: 0.3733\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 36s 2s/step - loss: 3.6539 - accuracy: 0.4212 - val_loss: 5.9795 - val_accuracy: 0.3728\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 41s 2s/step - loss: 3.4867 - accuracy: 0.4358 - val_loss: 6.0467 - val_accuracy: 0.3725\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 39s 2s/step - loss: 3.3408 - accuracy: 0.4529 - val_loss: 6.1165 - val_accuracy: 0.3748\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 38s 2s/step - loss: 3.2028 - accuracy: 0.4699 - val_loss: 6.2166 - val_accuracy: 0.3746\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 37s 2s/step - loss: 3.0698 - accuracy: 0.4859 - val_loss: 6.2378 - val_accuracy: 0.3715\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 39s 2s/step - loss: 2.9454 - accuracy: 0.5028 - val_loss: 6.3231 - val_accuracy: 0.3735\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 39s 2s/step - loss: 2.8289 - accuracy: 0.5198 - val_loss: 6.3650 - val_accuracy: 0.3715\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 36s 2s/step - loss: 2.7137 - accuracy: 0.5354 - val_loss: 6.4463 - val_accuracy: 0.3728\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 37s 2s/step - loss: 2.5932 - accuracy: 0.5520 - val_loss: 6.5076 - val_accuracy: 0.3719\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 38s 2s/step - loss: 2.4835 - accuracy: 0.5671 - val_loss: 6.5963 - val_accuracy: 0.3737\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 37s 2s/step - loss: 2.3890 - accuracy: 0.5807 - val_loss: 6.6492 - val_accuracy: 0.3720\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 36s 2s/step - loss: 2.2973 - accuracy: 0.5923 - val_loss: 6.7238 - val_accuracy: 0.3728\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 41s 2s/step - loss: 2.2018 - accuracy: 0.6089 - val_loss: 6.7728 - val_accuracy: 0.3732\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 38s 2s/step - loss: 2.1210 - accuracy: 0.6186 - val_loss: 6.8459 - val_accuracy: 0.3740\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 35s 2s/step - loss: 2.0449 - accuracy: 0.6279 - val_loss: 6.8896 - val_accuracy: 0.3729\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 35s 2s/step - loss: 1.9602 - accuracy: 0.6411 - val_loss: 6.9450 - val_accuracy: 0.3720\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 35s 2s/step - loss: 1.8896 - accuracy: 0.6527 - val_loss: 7.0044 - val_accuracy: 0.3724\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 45s 2s/step - loss: 1.8324 - accuracy: 0.6627 - val_loss: 7.0596 - val_accuracy: 0.3721\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 37s 2s/step - loss: 1.7722 - accuracy: 0.6699 - val_loss: 7.1180 - val_accuracy: 0.3729\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 43s 2s/step - loss: 1.7069 - accuracy: 0.6796 - val_loss: 7.1665 - val_accuracy: 0.3725\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 38s 2s/step - loss: 1.6516 - accuracy: 0.6893 - val_loss: 7.2225 - val_accuracy: 0.3728\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 34s 2s/step - loss: 1.6006 - accuracy: 0.6980 - val_loss: 7.2916 - val_accuracy: 0.3731\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 33s 2s/step - loss: 1.5506 - accuracy: 0.7058 - val_loss: 7.3477 - val_accuracy: 0.3728\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 35s 2s/step - loss: 1.5004 - accuracy: 0.7133 - val_loss: 7.3736 - val_accuracy: 0.3710\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 41s 2s/step - loss: 1.4629 - accuracy: 0.7184 - val_loss: 7.4387 - val_accuracy: 0.3726\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 37s 2s/step - loss: 1.5882 - accuracy: 0.7053 - val_loss: 7.6704 - val_accuracy: 0.3460\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 38s 2s/step - loss: 2.3388 - accuracy: 0.5580 - val_loss: 7.2423 - val_accuracy: 0.3684\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 33s 2s/step - loss: 1.7646 - accuracy: 0.6517 - val_loss: 7.2539 - val_accuracy: 0.3694\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 34s 2s/step - loss: 1.6414 - accuracy: 0.6768 - val_loss: 7.3181 - val_accuracy: 0.3700\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 34s 2s/step - loss: 1.5506 - accuracy: 0.6949 - val_loss: 7.3944 - val_accuracy: 0.3694\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 33s 2s/step - loss: 1.4829 - accuracy: 0.7066 - val_loss: 7.4396 - val_accuracy: 0.3713\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 33s 2s/step - loss: 1.4281 - accuracy: 0.7173 - val_loss: 7.5027 - val_accuracy: 0.3701\n"
     ]
    }
   ],
   "source": [
    "training = model.fit(x=[X_train, Y_train[:,:-1]],\n",
    "                     y=Y_train.reshape(Y_train.shape[0],\n",
    "                                       Y_train.shape[1],\n",
    "                                       1)[:,1:],\n",
    "                     batch_size=64,\n",
    "                     epochs=50,\n",
    "                     shuffle=True,\n",
    "                     verbose=1,\n",
    "                     validation_split=0.3,\n",
    "\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Model/model_gabungan_171223.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2d696cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 5s 284ms/step - loss: 6.4010 - accuracy: 0.4240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.401017665863037, 0.4239950478076935]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=[X_test, Y_test[:,:-1]],\n",
    "               y=Y_test.reshape(Y_test.shape[0],\n",
    "                                 Y_test.shape[1],\n",
    "                                 1)[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c552e510",
   "metadata": {},
   "source": [
    "## Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89598a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 800)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 800, 100)          5789600   \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(None, 800, 128),        117248    \n",
      "                              (None, 128),                       \n",
      "                              (None, 128)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5906848 (22.53 MB)\n",
      "Trainable params: 5906848 (22.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder Model\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c], name=\"Encoder_Model\")\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c1829f",
   "metadata": {},
   "source": [
    "## Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d737800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Decoder_Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, None, 100)            1478600   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, None, 128),          117248    ['embedding_2[0][0]',         \n",
      "                              (None, 128),                           'input_3[0][0]',             \n",
      "                              (None, 128)]                           'input_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 14785)          1907265   ['lstm_1[1][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3503113 (13.36 MB)\n",
      "Trainable params: 3503113 (13.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Decoder Model\n",
    "decoder_state_input_h = Input(shape=(lstm_units,))\n",
    "decoder_state_input_c = Input(shape=(lstm_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_embedding = Embedding(input_dim=sum_embeddings.shape[0], output_dim=embeddings_size,\n",
    "                              weights=[sum_embeddings[:, :embeddings_size]], trainable=True)(decoder_inputs)\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_outputs = dense_layer(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "    outputs=[decoder_outputs] + decoder_states,\n",
    "    name=\"Decoder_Model\"\n",
    ")\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f04eff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "encoder_model.save('Model/encoder_191223.h5')\n",
    "decoder_model.save('Model/decoder_191223.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dee95f",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e212abb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"amazing turn events baahubali 2 conclusion released worldwide april 28 beaten hollywood film circle us box office mind simple featthe circle stars twotime academy awardwinning actor global superstar tom hanks harry potter star heartthrob billions emma watson based novel name circle technothriller one awaited films yearhowever according latest boxoffice figures baahubali 2 given beating tom hanksstarrer us box office baahubali 2 conclusion earned  1013 million rs 65 crore first weekend circle earned  932 million rs 598 crorethis makes baahubali 2 conclusion directed ss rajamouli highest grossing indian film ever american soil us back home baahubali 2 conclusion also breaking records far hindi version film earned title highestgrossing film first weekend india beating sultan dangalstarring prabhas rana daggubati anushka shetty tamannaah sathyaraj ramya krishnan baahubali 2 conclusion fantasy epic sequel 2015 blockbuster baahubali beginning baahubali 2 expected earn whole lot time tell beat pk become world's highest grossing indian film time\"]\n"
     ]
    }
   ],
   "source": [
    "sample = x_test[1]\n",
    "\n",
    "clean_sample = re.sub(\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", sample)\n",
    "\n",
    "temp=\"\"\n",
    "text=clean_sample.split(\" \")\n",
    "for word in text:\n",
    "  if word not in stopword:\n",
    "    temp = temp+\" \"+word\n",
    "clean_sample = temp\n",
    "input_sample = []\n",
    "input_sample.append(clean_sample.strip())\n",
    "print(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b707454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_corpus.json') as f: \n",
    "        data = json.load(f) \n",
    "        test_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "input_seq = test_tokenizer.texts_to_sequences(input_sample)\n",
    "\n",
    "# Padding sequence\n",
    "x = tf.keras.utils.pad_sequences(input_seq, maxlen=800, padding='post', truncating=\"post\")\n",
    "# print(tokenizer.index_word)\n",
    "# print(art_text2seq)\n",
    "# print(x.shape)\n",
    "# print(x.shape[0], \"sequences of length\", x.shape[1])\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2d59a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " india captain england stealing killed flight plane contact next admissions admissions started end end end december reportedly reportedly police police police police\n"
     ]
    }
   ],
   "source": [
    "# Predict Manual\n",
    "# %xmode Verbose\n",
    "x = x.reshape(1, -1)\n",
    "\n",
    "# encode X\n",
    "encoder_out, state_h, state_c = encoder_model.predict(x, verbose=0)\n",
    "\n",
    "# prepare loop\n",
    "y_inp = np.array([y_tokenizer.word_index[special_tokens[1]]])\n",
    "y_expand = np.expand_dims(y_inp, axis=1)\n",
    "predicted_text = \"\"\n",
    "stop = False\n",
    "\n",
    "while not stop:\n",
    "    # predict dictionary probability distribution\n",
    "    outputs = decoder_model.predict([y_expand, state_h, state_c], verbose=0)\n",
    "    probs, new_state_h, new_state_c = outputs[0], outputs[1], outputs[2]\n",
    "\n",
    "    # get predicted word\n",
    "    voc_idx = np.argmax(probs[0, -1, :], axis=0)\n",
    "    if voc_idx == 0:\n",
    "        break\n",
    "\n",
    "    pred_word = y_tokenizer.index_word[voc_idx]\n",
    "\n",
    "    # check stop\n",
    "    if (pred_word != special_tokens[1]) and (len(predicted_text.split()) < 50):\n",
    "        predicted_text = predicted_text + \" \" + pred_word\n",
    "    else:\n",
    "        stop = True\n",
    "\n",
    "    # next\n",
    "    y_inp = np.array([voc_idx])\n",
    "    y_expand = np.expand_dims(y_inp, axis=1)\n",
    "    state_h, state_c = new_state_h, new_state_c\n",
    "\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e117c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = y_tokenizer.index_word\n",
    "reverse_source_word_index = x_tokenizer.index_word\n",
    "target_word_index = y_tokenizer.word_index\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq, verbose = 0)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['xstartx']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c], verbose = 0)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='xendx'):\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'xendx'  or len(decoded_sentence.split()) >= (49)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_sequence(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7a68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
